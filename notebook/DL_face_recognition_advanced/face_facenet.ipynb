{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸识别——通过facenet进行人脸识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进入ModelArts\n",
    "\n",
    "点击如下链接：https://www.huaweicloud.com/product/modelarts.html ， 进入ModelArts主页。点击“立即使用”按钮，输入用户名和密码登录，进入ModelArts使用页面。\n",
    "\n",
    "### 创建ModelArts notebook\n",
    "\n",
    "下面，我们在ModelArts中创建一个notebook开发环境，ModelArts notebook提供网页版的Python开发环境，可以方便的编写、运行代码，并查看运行结果。\n",
    "\n",
    "第一步：在ModelArts服务主界面依次点击“开发环境”、“创建”\n",
    "\n",
    "![create_nb_create_button](./img/create_nb_create_button.png)\n",
    "\n",
    "第二步：填写notebook所需的参数：\n",
    "\n",
    "|项目|建议填写方式|\n",
    "|-|-|\n",
    "|名称|自定义环境名称|\n",
    "|工作环境 | Python3|\n",
    "| 资源池 | 选择\"公共资源池\"即可 |\n",
    "|类型|GPU|\n",
    "|规格|GPU:1*p100, CPU:8核64GiB|\n",
    "|存储配置|EVS|\n",
    "|磁盘规格|5GB|\n",
    "\n",
    "第三步：配置好notebook参数后，点击下一步，进入notebook信息预览。确认无误后，点击“立即创建”\n",
    "\n",
    "![create_nb_creation_summary](./img/create_nb_creation_summary.png)\n",
    "\n",
    "第四步：创建完成后，返回开发环境主界面，等待Notebook创建完毕后，打开Notebook，进行下一步操作。\n",
    "![modelarts_notebook_index](./img/modelarts_notebook_index.png)\n",
    "\n",
    "### 在ModelArts中创建开发环境\n",
    "\n",
    "接下来，我们创建一个实际的开发环境，用于后续的实验步骤。\n",
    "\n",
    "第一步：点击下图所示的“打开”按钮，进入刚刚创建的Notebook\n",
    "![inter_dev_env](img/enter_dev_env.png)\n",
    "\n",
    "第二步：创建一个Python3环境的的Notebook。点击右上角的\"New\"，然后选择TensorFlow 1.13.1开发环境。\n",
    "\n",
    "第三步：点击左上方的文件名\"Untitled\"，并输入一个与本实验相关的名称，如\"facenet\"\n",
    "![notebook_untitled_filename](./img/notebook_untitled_filename.png)\n",
    "![notebook_name_the_ipynb](./img/notebook_name_the_ipynb.png)\n",
    "\n",
    "\n",
    "### 在Notebook中编写并执行代码\n",
    "\n",
    "在Notebook中，我们输入一个简单的打印语句，然后点击上方的运行按钮，可以查看语句执行的结果：\n",
    "![run_helloworld](./img/run_helloworld.png)\n",
    "\n",
    "\n",
    "开发环境准备好啦，接下来可以愉快地写代码啦！\n",
    "\n",
    "### 数据和代码下载\n",
    "运行下面代码，进行数据和代码的下载和解压\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully download file modelarts-labs/notebook/DL_face_recognition_advanced/face.tar.gz from OBS to local ./face.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from modelarts.session import Session\n",
    "sess = Session()\n",
    "\n",
    "if sess.region_name == 'cn-north-1':\n",
    "    bucket_path=\"modelarts-labs/notebook/DL_face_recognition_advanced/face.tar.gz\"\n",
    "elif sess.region_name == 'cn-north-4':\n",
    "    bucket_path=\"modelarts-labs-bj4/notebook/DL_face_recognition_advanced/face.tar.gz\"\n",
    "else:\n",
    "    print(\"请更换地区到北京一或北京四\")\n",
    "\n",
    "sess.download_data(bucket_path=bucket_path, path=\"./face.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解压文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf face.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始案例，首先，对FaceNet进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from src.facenet import get_dataset, load_data, load_model\n",
    "from src.train_tripletloss import sample_people, select_triplets, save_variables_and_metagraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建facenet模型结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception-Resnet-A\n",
    "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
    "    \"\"\"Builds the 35x35 resnet block.\"\"\"\n",
    "    with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n",
    "        with tf.variable_scope('Branch_2'):\n",
    "            tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope='Conv2d_0b_3x3')\n",
    "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope='Conv2d_0c_3x3')\n",
    "        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
    "                         activation_fn=None, scope='Conv2d_1x1')\n",
    "        net += scale * up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception-Resnet-B\n",
    "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
    "    \"\"\"Builds the 17x17 resnet block.\"\"\"\n",
    "    with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            tower_conv = slim.conv2d(net, 128, 1, scope='Conv2d_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7], scope='Conv2d_0b_1x7')\n",
    "            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1], scope='Conv2d_0c_7x1')\n",
    "        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1')\n",
    "        net += scale * up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception-Resnet-C\n",
    "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
    "    \"\"\"Builds the 8x8 resnet block.\"\"\"\n",
    "    with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n",
    "                                        scope='Conv2d_0b_1x3')\n",
    "            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n",
    "                                        scope='Conv2d_0c_3x1')\n",
    "        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
    "                         activation_fn=None, scope='Conv2d_1x1')\n",
    "        net += scale * up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_a(net, k, l, m, n):\n",
    "    with tf.variable_scope('Branch_0'):\n",
    "        tower_conv = slim.conv2d(net, n, 3, stride=2, padding='VALID',\n",
    "                                 scope='Conv2d_1a_3x3')\n",
    "    with tf.variable_scope('Branch_1'):\n",
    "        tower_conv1_0 = slim.conv2d(net, k, 1, scope='Conv2d_0a_1x1')\n",
    "        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n",
    "                                    scope='Conv2d_0b_3x3')\n",
    "        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n",
    "                                    stride=2, padding='VALID',\n",
    "                                    scope='Conv2d_1a_3x3')\n",
    "    with tf.variable_scope('Branch_2'):\n",
    "        tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
    "                                     scope='MaxPool_1a_3x3')\n",
    "    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_b(net):\n",
    "    with tf.variable_scope('Branch_0'):\n",
    "        tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n",
    "                                   padding='VALID', scope='Conv2d_1a_3x3')\n",
    "    with tf.variable_scope('Branch_1'):\n",
    "        tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n",
    "                                    padding='VALID', scope='Conv2d_1a_3x3')\n",
    "    with tf.variable_scope('Branch_2'):\n",
    "        tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n",
    "                                    scope='Conv2d_0b_3x3')\n",
    "        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n",
    "                                    padding='VALID', scope='Conv2d_1a_3x3')\n",
    "    with tf.variable_scope('Branch_3'):\n",
    "        tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
    "                                     scope='MaxPool_1a_3x3')\n",
    "    net = tf.concat([tower_conv_1, tower_conv1_1,\n",
    "                        tower_conv2_2, tower_pool], 3)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# from src.models.inception_resnet_v1 import *\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def inception_resnet_v1(inputs, is_training=True,\n",
    "                        dropout_keep_prob=0.8,\n",
    "                        bottleneck_layer_size=128,\n",
    "                        reuse=None, \n",
    "                        scope='InceptionResnetV1'):\n",
    "    end_points = {}\n",
    "  \n",
    "    with tf.variable_scope(scope, 'InceptionResnetV1', [inputs], reuse=reuse):\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout],is_training=is_training):\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "                # stem 部分\n",
    "                # 149 x 149 x 32\n",
    "                net = slim.conv2d(inputs, 32, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                end_points['Conv2d_1a_3x3'] = net\n",
    "                # 147 x 147 x 32\n",
    "                net = slim.conv2d(net, 32, 3, padding='VALID', scope='Conv2d_2a_3x3')\n",
    "                end_points['Conv2d_2a_3x3'] = net \n",
    "                # 147 x 147 x 64\n",
    "                net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n",
    "                end_points['Conv2d_2b_3x3'] = net                \n",
    "                # 73 x 73 x 64\n",
    "                net = slim.max_pool2d(net, 3, stride=2, padding='VALID', scope='MaxPool_3a_3x3')\n",
    "                end_points['MaxPool_3a_3x3'] = net                \n",
    "                # 73 x 73 x 80\n",
    "                net = slim.conv2d(net, 80, 1, padding='VALID', scope='Conv2d_3b_1x1')\n",
    "                end_points['Conv2d_3b_1x1'] = net\n",
    "                # 71 x 71 x 192\n",
    "                net = slim.conv2d(net, 192, 3, padding='VALID', scope='Conv2d_4a_3x3')\n",
    "                end_points['Conv2d_4a_3x3'] = net\n",
    "                # 35 x 35 x 256\n",
    "                net = slim.conv2d(net, 256, 3, stride=2, padding='VALID', scope='Conv2d_4b_3x3')\n",
    "                end_points['Conv2d_4b_3x3'] = net\n",
    "\n",
    "                # Inception-resnet-A 部分\n",
    "                net = slim.repeat(net, 5, block35, scale=0.17)\n",
    "                end_points['Mixed_5a'] = net\n",
    "        \n",
    "                # Reduction-A\n",
    "                with tf.variable_scope('Mixed_6a'):\n",
    "                    net = reduction_a(net, 192, 192, 256, 384)\n",
    "                end_points['Mixed_6a'] = net\n",
    "                \n",
    "                # Inception-Resnet-B 部分\n",
    "                net = slim.repeat(net, 10, block17, scale=0.10)\n",
    "                end_points['Mixed_6b'] = net\n",
    "                \n",
    "                # Reduction-B\n",
    "                with tf.variable_scope('Mixed_7a'):\n",
    "                    net = reduction_b(net)\n",
    "                end_points['Mixed_7a'] = net\n",
    "                \n",
    "                # Inception-Resnet-C 部分\n",
    "                net = slim.repeat(net, 5, block8, scale=0.20)\n",
    "                end_points['Mixed_8a'] = net\n",
    "                \n",
    "                net = block8(net, activation_fn=None)\n",
    "                end_points['Mixed_8b'] = net\n",
    "                \n",
    "                with tf.variable_scope('Logits'):\n",
    "                    end_points['PrePool'] = net\n",
    "                    #pylint: disable=no-member\n",
    "                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n",
    "                                          scope='AvgPool_1a_8x8')\n",
    "                    net = slim.flatten(net)\n",
    "          \n",
    "                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='Dropout')\n",
    "          \n",
    "                    end_points['PreLogitsFlatten'] = net\n",
    "                \n",
    "                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n",
    "                        scope='Bottleneck', reuse=False)\n",
    "  \n",
    "    return net, end_points\n",
    "tf.reset_default_graph()\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, 229, 229, 3))\n",
    "net, end_points = inception_resnet_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到各节点信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conv2d_1a_3x3': <tf.Tensor 'InceptionResnetV1/Conv2d_1a_3x3/Relu:0' shape=(?, 114, 114, 32) dtype=float32>,\n",
       " 'Conv2d_2a_3x3': <tf.Tensor 'InceptionResnetV1/Conv2d_2a_3x3/Relu:0' shape=(?, 112, 112, 32) dtype=float32>,\n",
       " 'Conv2d_2b_3x3': <tf.Tensor 'InceptionResnetV1/Conv2d_2b_3x3/Relu:0' shape=(?, 112, 112, 64) dtype=float32>,\n",
       " 'Conv2d_3b_1x1': <tf.Tensor 'InceptionResnetV1/Conv2d_3b_1x1/Relu:0' shape=(?, 55, 55, 80) dtype=float32>,\n",
       " 'Conv2d_4a_3x3': <tf.Tensor 'InceptionResnetV1/Conv2d_4a_3x3/Relu:0' shape=(?, 53, 53, 192) dtype=float32>,\n",
       " 'Conv2d_4b_3x3': <tf.Tensor 'InceptionResnetV1/Conv2d_4b_3x3/Relu:0' shape=(?, 26, 26, 256) dtype=float32>,\n",
       " 'MaxPool_3a_3x3': <tf.Tensor 'InceptionResnetV1/MaxPool_3a_3x3/MaxPool:0' shape=(?, 55, 55, 64) dtype=float32>,\n",
       " 'Mixed_5a': <tf.Tensor 'InceptionResnetV1/Repeat/block35_5/Relu:0' shape=(?, 26, 26, 256) dtype=float32>,\n",
       " 'Mixed_6a': <tf.Tensor 'InceptionResnetV1/Mixed_6a/concat:0' shape=(?, 12, 12, 896) dtype=float32>,\n",
       " 'Mixed_6b': <tf.Tensor 'InceptionResnetV1/Repeat_1/block17_10/Relu:0' shape=(?, 12, 12, 896) dtype=float32>,\n",
       " 'Mixed_7a': <tf.Tensor 'InceptionResnetV1/Mixed_7a/concat:0' shape=(?, 5, 5, 1792) dtype=float32>,\n",
       " 'Mixed_8a': <tf.Tensor 'InceptionResnetV1/Repeat_2/block8_5/Relu:0' shape=(?, 5, 5, 1792) dtype=float32>,\n",
       " 'Mixed_8b': <tf.Tensor 'InceptionResnetV1/Block8/add:0' shape=(?, 5, 5, 1792) dtype=float32>,\n",
       " 'PreLogitsFlatten': <tf.Tensor 'InceptionResnetV1/Logits/Dropout/dropout_1/mul:0' shape=(?, 1792) dtype=float32>,\n",
       " 'PrePool': <tf.Tensor 'InceptionResnetV1/Block8/add:0' shape=(?, 5, 5, 1792) dtype=float32>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, keep_probability, phase_train=True,\n",
    "              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n",
    "    batch_norm_params = {\n",
    "        # Decay for the moving averages.\n",
    "        'decay': 0.995,\n",
    "        # epsilon to prevent 0s in variance.\n",
    "        'epsilon': 0.001,\n",
    "        # force in-place updates of mean and variance estimates\n",
    "        'updates_collections': None,\n",
    "        # Moving averages ends up in the trainable variables collection\n",
    "        'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n",
    "    }\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        weights_initializer=slim.initializers.xavier_initializer(),\n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                        normalizer_fn=slim.batch_norm,\n",
    "                        normalizer_params=batch_norm_params):\n",
    "        return inception_resnet_v1(images, is_training=phase_train,\n",
    "              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个新的Graph，全局步数为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建变量的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')\n",
    "batch_size_placeholder = tf.placeholder(tf.int32, name='batch_size')\n",
    "phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n",
    "image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name='image_paths')\n",
    "labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置超参，可以根据不同情况进行调节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据路径\n",
    "data_dir = './data/CASIA-maxpy-clean/'\n",
    "# 预训练模型位置\n",
    "pretrained_model = \"./20170512-110547/\"\n",
    "# 图片大小\n",
    "image_size  =160\n",
    "# 批大小\n",
    "batch_size = 90\n",
    "# Embedding维度\n",
    "embedding_size = 128\n",
    "# 权重衰减\n",
    "weight_decay = 0.0\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.05\n",
    "# 批处理数量\n",
    "people_per_batch = 9\n",
    "images_per_person = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据准备部分\n",
    "\n",
    "创建enqueue_op，创建数据多线程读取节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n",
    "                        dtypes=[tf.string, tf.int64],\n",
    "                        shapes=[(3,), (3,)],\n",
    "                        shared_name=None, name=None)\n",
    "enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在FaceNet中，图像预处理经过了数据集增强：随机切割，随机翻转，以及图片标准化，具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-16-888c8bd4f082>:33: batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/training/input.py:736: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/training/input.py:736: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/training/input.py:823: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "images_and_labels = []\n",
    "# 读取数据线程数\n",
    "nrof_preprocess_threads = 4\n",
    "for _ in range(nrof_preprocess_threads):\n",
    "#     在队列中将数据读取出来\n",
    "    filenames, label = input_queue.dequeue()\n",
    "    images = []\n",
    "    for filename in tf.unstack(filenames):\n",
    "        #读取图片并对图片进行编码\n",
    "        file_contents = tf.read_file(filename)\n",
    "        image = tf.image.decode_image(file_contents, channels=3)\n",
    "        random_crop = True\n",
    "        #随机切割：一种数据扩充方法，不但提高了模型精度，也增强了模型稳定性。\n",
    "        if random_crop:\n",
    "            image = tf.random_crop(image, [image_size, image_size, 3])\n",
    "        else:\n",
    "            image = tf.image.resize_image_with_crop_or_pad(image, image_size, image_size)\n",
    "        random_flip = True\n",
    "        #图片随机翻转：数据扩充\n",
    "        if random_flip:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "        #对图片大小进行统一\n",
    "        image.set_shape((image_size, image_size, 3))\n",
    "        # 对图片进行标准化，以提高训练的速度\n",
    "        images.append(tf.image.per_image_standardization(image))\n",
    "    images_and_labels.append([images, label])\n",
    "# 多线程queue，将数据进行批读取\n",
    "\n",
    "image_batch, labels_batch = tf.train.batch_join(\n",
    "        images_and_labels, batch_size=batch_size_placeholder, \n",
    "        shapes=[(image_size, image_size, 3), ()], enqueue_many=True,\n",
    "        capacity=4 * nrof_preprocess_threads * batch_size,\n",
    "        allow_smaller_final_batch=True)\n",
    "# 赋值语句\n",
    "image_batch = tf.identity(image_batch, 'image_batch')\n",
    "image_batch = tf.identity(image_batch, 'input')\n",
    "labels_batch = tf.identity(labels_batch, 'label_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载Inception-ResnetV1网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# droupout概率\n",
    "keep_probability = 1.0\n",
    "#加载 Inception-ResnetV1网络结构\n",
    "prelogits, _ = inference(image_batch, keep_probability, phase_train=phase_train_placeholder, \n",
    "                                 bottleneck_layer_size=embedding_size,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对神经网络的计算结果进行L2范式归一化，然后进行三重损失函数计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将embeddings分为 anchor， positive和 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三重损失计算函数`triplet_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "def triplet_loss(anchor, positive, negative, alpha):\n",
    "    with tf.variable_scope('triplet_loss'):\n",
    "        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
    "        \n",
    "        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n",
    "        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
    "      \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据anchor， positive和 negative计算三重损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = triplet_loss(anchor, positive, negative, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算损失函数总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有损失\n",
    "regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "total_loss = tf.add_n([triplet_loss] + regularization_losses, name='total_loss')\n",
    "#  生成所有损失的滑动平均\n",
    "loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "losses = tf.get_collection('losses')\n",
    "loss_averages_op = loss_averages.apply(losses + [total_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义学习率衰减方法：学习率指数衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率衰减步数\n",
    "learning_rate_decay_epochs = 100\n",
    "# 步大小\n",
    "epoch_size = 10\n",
    "# 学习率衰减因数\n",
    "learning_rate_decay_factor = 1.0\n",
    "# 学习率指数衰减\n",
    "learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,learning_rate_decay_epochs*epoch_size,\n",
    "                                           learning_rate_decay_factor, staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 优化器\n",
    "可以看到在下面提供了几种不同的优化器可供选择，分别为ADAGRAD，ADADELTA，ADAM，RMSPROP和MOM。优化器的具体实现已经由TensorFlow封装起来，可以直接进行调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器类型\n",
    "optimizer = 'ADAGRAD'\n",
    "\n",
    "with tf.control_dependencies([loss_averages_op]):\n",
    "    if optimizer=='ADAGRAD':\n",
    "        opt = tf.train.AdagradOptimizer(learning_rate)\n",
    "    elif optimizer=='ADADELTA':\n",
    "        opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n",
    "    elif optimizer=='ADAM':\n",
    "        opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n",
    "    elif optimizer=='RMSPROP':\n",
    "        opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n",
    "    elif optimizer=='MOM':\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimization algorithm')\n",
    "# 计算梯度\n",
    "    grads = opt.compute_gradients(total_loss, tf.global_variables())\n",
    "# 计算梯度节点\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义train节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 衰减率\n",
    "moving_average_decay = 0.999\n",
    "\n",
    "# 追踪所有变量的移动平均，消除特殊值的影响\n",
    "variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "# 将梯度计算和移动平均加入train节点\n",
    "with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-800be5e9a544>:17: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-batch_join/fifo_queue-batch_join/fifo_queue_EnqueueMany, started daemon 139837415876352)>,\n",
       " <Thread(QueueRunnerThread-batch_join/fifo_queue-batch_join/fifo_queue_EnqueueMany_1, started daemon 139837847738112)>,\n",
       " <Thread(QueueRunnerThread-batch_join/fifo_queue-batch_join/fifo_queue_EnqueueMany_2, started daemon 139837856130816)>,\n",
       " <Thread(QueueRunnerThread-batch_join/fifo_queue-batch_join/fifo_queue_EnqueueMany_3, started daemon 139837881308928)>,\n",
       " <Thread(QueueRunnerThread-batch_join/fifo_queue-close_on_stop, started daemon 139837866026752)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取训练数据\n",
    "train_set = get_dataset(data_dir)\n",
    "\n",
    "# 创建模型saver\n",
    "saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n",
    "\n",
    "# 设置session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n",
    "\n",
    "# TensorFlow 变量初始化\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n",
    "sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n",
    "\n",
    "# 创建一个多线程管理器\n",
    "coord = tf.train.Coordinator()\n",
    "tf.train.start_queue_runners(coord=coord, sess=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程使用了CASIA数据集\n",
    "\n",
    ">CASIA数据集    \n",
    "数据集提供了约10000个对象的约500000张图片，叫做CASIA- WebFace. 借用iMDb网站抓取人脸数据并进行标记，采用了一种半自动的数据收集方法，避免了逐一为数据增加标记。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate, batch_size , image_size, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n",
    "          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n",
    "          embeddings, loss, train_op, embedding_size, anchor, positive, negative, triplet_loss):\n",
    "    \n",
    "    batch_number = 0\n",
    "    lr = learning_rate\n",
    "    print(lr)\n",
    "    # 进行epoch 数量的训练    \n",
    "    while batch_number < epoch_size:\n",
    "        # 进行数据随机挑选\n",
    "        image_paths, num_per_class = sample_people(dataset, people_per_batch, images_per_person)\n",
    "        # 加载数据集中的图片路径和数量信息\n",
    "        print('Running forward pass on sampled images: ', end='')\n",
    "        start_time = time.time()\n",
    "        nrof_examples = people_per_batch * images_per_person\n",
    "        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n",
    "        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n",
    "        # 进行数据加载操作\n",
    "        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n",
    "        emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "        nrof_batches = int(np.ceil(nrof_examples / batch_size))\n",
    "        for i in range(nrof_batches):\n",
    "            batch_size = min(nrof_examples-i*batch_size, batch_size)\n",
    "\n",
    "            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n",
    "                learning_rate_placeholder: lr, phase_train_placeholder: True})\n",
    "            emb_array[lab,:] = emb\n",
    "        print('%.3f' % (time.time()-start_time))\n",
    "\n",
    "#         根据embedding的数值选择triplets\n",
    "        print('Selecting suitable triplets for training')\n",
    "        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n",
    "            image_paths, people_per_batch, alpha)\n",
    "        selection_time = time.time() - start_time\n",
    "        print('(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds' % \n",
    "            (nrof_random_negs, nrof_triplets, selection_time))\n",
    "\n",
    "        nrof_batches = int(np.ceil(nrof_triplets*3/batch_size))\n",
    "        triplet_paths = list(itertools.chain(*triplets))\n",
    "        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n",
    "        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n",
    "        \n",
    "#         将triplets进行训练\n",
    "        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n",
    "        nrof_examples = len(triplet_paths)\n",
    "        train_time = 0\n",
    "        i = 0\n",
    "        emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "        loss_array = np.zeros((nrof_triplets,))\n",
    "        step = 0\n",
    "        while i < nrof_batches:\n",
    "            start_time = time.time()\n",
    "            batch_size = min(nrof_examples-i*batch_size, batch_size)\n",
    "            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n",
    "#             训练train_op\n",
    "            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n",
    "            emb_array[lab,:] = emb\n",
    "            loss_array[i] = err\n",
    "            duration = time.time() - start_time\n",
    "            print('Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f' %\n",
    "                  (epoch, batch_number+1, epoch_size, duration, err))\n",
    "            batch_number += 1\n",
    "            i += 1\n",
    "            train_time += duration\n",
    "\n",
    "    return step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: ./20170512-110547/\n",
      "Metagraph file: model-20170512-110547.meta\n",
      "Checkpoint file: model-20190614-022028.ckpt-768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The saved meta_graph is possibly from an older release:\n",
      "'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "Running forward pass on sampled images: 6.552\n",
      "Selecting suitable triplets for training\n",
      "(nrof_random_negs, nrof_triplets) = (252, 130): time=6.561 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/src/train_tripletloss.py:299: RuntimeWarning: invalid value encountered in less\n",
      "  all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os \n",
    "subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "\n",
    "with sess.as_default():\n",
    "    load_model(pretrained_model)\n",
    "    epoch = 0\n",
    "    while epoch < 5:\n",
    "        step = sess.run(global_step, feed_dict=None)\n",
    "        epoch = step // epoch_size\n",
    "        # 调用训练函数进行训练过程\n",
    "        # 调用一次train函数是一个轮次\n",
    "        train(0.05, batch_size, image_size, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n",
    "            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n",
    "            embeddings, total_loss, train_op, \n",
    "            embedding_size, anchor, positive, negative, triplet_loss)\n",
    "\n",
    "        checkpoint_path = os.path.join(pretrained_model, 'model-%s.ckpt' % subdir)\n",
    "        saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸识别测试\n",
    "\n",
    "我们已经了解了MTCNN和FaceNet的原理，对FaceNet模型进行了训练，接下来我们进行人脸识别的测试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个案例中，我们将让计算机来分辨欧阳妮妮，欧阳娜娜以及欧阳娣娣三姐妹，为了体现测试数据的多样性，我们加入腾格尔作为对照。\n",
    "\n",
    "我们首先找到两张欧阳娜娜的照片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import scipy.misc as misc\n",
    "import copy\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import src.facenet as facenet\n",
    "import src.align.detect_face as detect_face\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数show_images 负责将图片显示出来\n",
    "# images_list 图片路径集合\n",
    "# images_names 图片名称集合，顺序与大小与图片路径保持一致\n",
    "def show_images(images_list, images_names):\n",
    "    if len(images_list) != len(images_names):\n",
    "        print(\"图片名和图片数量不符\")\n",
    "    else:\n",
    "        l = len(images_list)\n",
    "        #确定行列数\n",
    "        #固定为2列\n",
    "        line =  math.ceil(l/2)\n",
    "        for i in range(l):\n",
    "            plt.rcParams['savefig.dpi'] = 200 #图片像素\n",
    "            plt.rcParams['figure.dpi'] = 150 #分辨率\n",
    "            plt.subplot(line,2,i+1)\n",
    "            plt.imshow(Image.open(images_list[i]))\n",
    "            plt.title(images_names[i],fontsize = 10)\n",
    "            plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list =  [\"./img_nana_1.jpg\",\"./img_nana_2.jpeg\"]\n",
    "images_names = [\"nana_1\",\"nana_2\"]\n",
    "show_images(images_list,images_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们找到欧阳娜娜的姐妹：欧阳妮妮和欧阳娣娣的照片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list =  [\"./img_nini.jpeg\",\"./img_didi.jpeg\"]\n",
    "images_names = [\"nini\",\"didi\"]\n",
    "show_images(images_list,images_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么我们也找一张腾格尔的照片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_list =  [\"./img_3.jpeg\"]\n",
    "images_names = [\"tenggeer\"]\n",
    "show_images(images_list,images_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始进行人脸比对，首先将人脸区域划分出来。将所有的文件路径加入到image_path中，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = [\"./img_nana_1.jpg\",\"./img_nana_2.jpeg\",\"./img_nini.jpeg\",\"./img_didi.jpeg\",\"./img_3.jpeg\"]\n",
    "image_name = [\"nana_1\",\"nana_2\",\"nini\",\"didi\",\"tenggeer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行人脸区域检测。人脸区域检测模型MTCNN，分为PNet， RNet以及ONet， 三种卷积神经网络，加入NMS，完成对人脸的检测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsize = 20 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "tmp_image_paths=copy.copy(image_path)\n",
    "img_list = [] \n",
    "mtcnn_list = []\n",
    "with tf.Graph().as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "    with sess.as_default():\n",
    "        pnet, rnet, onet = detect_face.create_mtcnn(sess, None)\n",
    "margin = 44\n",
    "image_size = 160\n",
    "for image in tmp_image_paths:\n",
    "#     img = misc.imread(os.path.expanduser(image), mode='RGB')\n",
    "    img = Image.open(image)\n",
    "    img = np.array(img)\n",
    "    img_size = np.asarray(img.shape)[0:2]\n",
    "    bounding_boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "    if len(bounding_boxes) < 1:\n",
    "        image_paths.remove(image)\n",
    "        continue\n",
    "    det = np.squeeze(bounding_boxes[0,0:4])\n",
    "    bb = np.zeros(4, dtype=np.int32)\n",
    "    bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "    bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "    bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n",
    "    bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n",
    "    cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "    mtcnn_list.append(cropped)\n",
    "    aligned = misc.imresize(cropped, (image_size, image_size), interp='bilinear')\n",
    "    prewhitened = facenet.prewhiten(aligned)\n",
    "    img_list.append(prewhitened)\n",
    "images = np.stack(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人脸检测结果如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(mtcnn_list)\n",
    "line =  1\n",
    "for i in range(l):\n",
    "    plt.rcParams['savefig.dpi'] = 100 #图片像素\n",
    "    plt.rcParams['figure.dpi'] = 150 #分辨率\n",
    "    plt.subplot(1,l,i+1)\n",
    "    plt.imshow(Image.fromarray(mtcnn_list[i]))\n",
    "    plt.axis('off')\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将人脸检测结果放入FaceNet模型，得到预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        facenet.load_model(\"./20170512-110547/20170512-110547.pb\")\n",
    "        images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "        \n",
    "        feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n",
    "        emb = sess.run(embeddings, feed_dict=feed_dict)\n",
    "        nrof_images = len(image_path)\n",
    "\n",
    "        data = []\n",
    "        data_item = []\n",
    "        for i in range(nrof_images):\n",
    "            for j in range(nrof_images):\n",
    "                if j<i:\n",
    "                    data_item.append(\"\"+str(image_name[i])+\"&\"+str(image_name[j]))\n",
    "                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\n",
    "                    data.append(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将运行结果绘制出来，可以看到数值最低的是娜娜本人的两张照片，姐妹照片的数值处于中间值，而数值最高的是腾格尔的照片与三姐妹的照片相比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "index = 0\n",
    "for face_0 in range(len(mtcnn_list)):\n",
    "    for face_1 in range(len(mtcnn_list)):\n",
    "        if face_1<face_0:\n",
    "            score = data[index]\n",
    "\n",
    "            plt.rcParams['savefig.dpi'] = 65 #图片像素\n",
    "            plt.rcParams['figure.dpi'] = 65 #分辨率\n",
    "\n",
    "            fig = plt.figure(constrained_layout=False)\n",
    "            spec_0 = gridspec.GridSpec(ncols=3, nrows=1, figure=fig)\n",
    "\n",
    "            f0_pic1 = fig.add_subplot(spec_0[0, 0])\n",
    "            f0_pic1.imshow(Image.fromarray(mtcnn_list[face_0]))\n",
    "            f0_pic1.axis('off')\n",
    "            f0_score = fig.add_subplot(spec_0[0, 1])\n",
    "            multi = math.floor(score/0.1)\n",
    "            rema = score%0.1\n",
    "            # 整0.1\n",
    "            for i in range(multi):\n",
    "                if i ==0 :\n",
    "                    p = f0_score.bar(np.array(1), (0.1), 0.1,  color = (1,0,0.2))\n",
    "                else:\n",
    "                    p = f0_score.bar(np.array(1), (0.1), 0.1, bottom=(0.1*(i)), color = (1-0.05*(i),0+0.05*(i),0.2))\n",
    "            # 0.1余数\n",
    "            p = f0_score.bar(np.array(1), (rema), 0.1, bottom=(0.1*multi), color = (1-0.05*(multi+1),0+0.05*(multi+1),0.2))\n",
    "            # 加入悬浮数字\n",
    "            offset = {'center': 0.5, 'right': 0.57, 'left': 0.43} \n",
    "            ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n",
    "            height = data[index]\n",
    "            for bar in p:\n",
    "                f0_score.text(bar.get_x() + bar.get_width()*offset['center'], 1.01*height,\n",
    "                        '{}'.format(height), ha=ha['center'], va='bottom')\n",
    "            f0_score.axis('off')\n",
    "            f2_pic1 = fig.add_subplot(spec_0[0, 2])\n",
    "            f2_pic1.imshow(Image.fromarray(mtcnn_list[face_1]))\n",
    "            f2_pic1.axis('off')\n",
    "            index = index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中表示两两对比中，数字越大代表人脸间差别越大，差别越大，色柱颜色变化越大。可以看到，\n",
    "娜娜本人照片色柱变化最小，数值也最小，\n",
    "三姐妹之间的色柱颜色几乎变化不大，数值大概为1.0左右，\n",
    "而腾格尔作为性别，年龄都差别很大的对照组，色柱变化最大，数值也到了1.5左右。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.13.1",
   "language": "python",
   "name": "tensorflow-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
